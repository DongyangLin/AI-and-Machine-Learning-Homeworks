{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 7\n",
    "12110418 庄子鲲\n",
    "### Problem 1\n",
    "Using Python and Numpy, write a class named DecisionTree, which implements the decision tree model based on information gain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Calculate the information entropy:\n",
    "$$\n",
    "H(Y|X=x)=-\\sum_{y\\in{Y}}{P(y|x)\\log_{2}{P(y|x)}}\n",
    "$$\n",
    "2. Calculate the imformation gain:\n",
    "$$\n",
    "IG(Y)=H(Y)-H(Y|X)\n",
    "$$\n",
    "3. Choose the feature that has maximum IG as the root node\n",
    "4. Continue calculate the information gain of the rest features and choose the one with the largest IG.\n",
    "5. If the output are the same, stop gorwing that branch.\n",
    "6. Complete the decision tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "class DecisionTree:\n",
    "    def __init__(self, max_depth=None):\n",
    "        self.max_depth = max_depth\n",
    "        self.tree = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.tree = self._grow_tree(X, y)\n",
    "\n",
    "    def _calculate_entropy(self, y):\n",
    "        unique_classes, class_counts = np.unique(y, return_counts=True)\n",
    "        probabilities = class_counts / len(y)\n",
    "        entropy = -np.sum(probabilities * np.log2(probabilities))\n",
    "        return entropy\n",
    "\n",
    "    def _calculate_information_gain(self, X, y, feature_index):\n",
    "        unique_features, featrue_counts = np.unique(X[:, feature_index],return_counts=True)\n",
    "        mask=[]\n",
    "        for i in range(len(unique_features)):\n",
    "            mask.append(X[:, feature_index] == unique_features[i])\n",
    "        # Calculate entropy for the two subsets\n",
    "        entropy_parent = self._calculate_entropy(y)\n",
    "        entropy_leaves=[]\n",
    "        for i in range(len(mask)):\n",
    "            entropy_leaves.append(self._calculate_entropy(y[mask[i]]))\n",
    "        # Calculate information gain\n",
    "        size=[]\n",
    "        total_size=0\n",
    "        for i in range(len(mask)):\n",
    "            size.append(np.sum(mask[i]))\n",
    "            total_size+=np.sum(mask[i])\n",
    "\n",
    "        information_gain = entropy_parent\n",
    "        \n",
    "        for i in range(len(mask)):\n",
    "            information_gain -= (size[i] / total_size) * entropy_leaves[i]\n",
    "        return information_gain\n",
    "    \n",
    "    # Find the best feature to split on\n",
    "    def _find_best_split(self, X, y):\n",
    "        num_samples, num_features = X.shape\n",
    "        best_information_gain = -1\n",
    "        best_split = None\n",
    "        \n",
    "        for feature_index in range(num_features):\n",
    "            information_gain = self._calculate_information_gain(X, y, feature_index)\n",
    "            if information_gain > best_information_gain:\n",
    "                best_information_gain = information_gain\n",
    "                best_split = feature_index\n",
    "        return best_split\n",
    "\n",
    "    def _grow_tree(self, X, y, depth=0):\n",
    "        # Check termination conditions\n",
    "        if depth == self.max_depth or np.all(y == y[0]):\n",
    "            return {\"class\": np.argmax(np.bincount(y)), \"depth\": depth}\n",
    "\n",
    "        # Find the best feature and split on it\n",
    "        best_split = self._find_best_split(X, y)\n",
    "        if best_split is None:\n",
    "            return {\"class\": np.argmax(np.bincount(y)), \"depth\": depth}\n",
    "        \n",
    "        feature_index = best_split\n",
    "        mask=[]\n",
    "        unique_features, featrue_counts = np.unique(X[:, feature_index],return_counts=True)\n",
    "        for i in range(len(unique_features)):\n",
    "            mask.append(X[:, feature_index] == unique_features[i])\n",
    "            \n",
    "        # Grow the subtrees\n",
    "        subtree=[]\n",
    "        for i in range(len(mask)):\n",
    "            subtree.append(self._grow_tree(X[mask[i]], y[mask[i]], depth + 1))\n",
    "        \n",
    "        return{\n",
    "            \"feature_index\": feature_index,\n",
    "            \"values\":unique_features,\n",
    "            \"subtree\": subtree,\n",
    "            \"depth\": depth,\n",
    "        }   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2:\n",
    "Consider the dataset lenses.data (the link is also provided) and use the DecisionTree class to build a decsion tree for this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree: \n",
      "Feature index: 3, values: [1 2]\n",
      "  value 1:   Output: 3\n",
      "  value 2:   Feature index: 2, values: [1 2]\n",
      "    value 1:     Feature index: 0, values: [1 2 3]\n",
      "      value 1:       Output: 2\n",
      "      value 2:       Output: 2\n",
      "      value 3:       Feature index: 1, values: [1 2]\n",
      "        value 1:         Output: 3\n",
      "        value 2:         Output: 2\n",
      "    value 2:     Feature index: 1, values: [1 2]\n",
      "      value 1:       Output: 1\n",
      "      value 2:       Feature index: 0, values: [1 2 3]\n",
      "        value 1:         Output: 1\n",
      "        value 2:         Output: 3\n",
      "        value 3:         Output: 3\n"
     ]
    }
   ],
   "source": [
    "# Load Data and Grow the Decision Tree\n",
    "lenses=np.loadtxt('lenses.data',usecols=(1,2,3,4,5))\n",
    "lenses = lenses.astype('int64', casting='unsafe')\n",
    "X_train=lenses[:, :4]\n",
    "y_train=lenses[:,-1]\n",
    "model = DecisionTree(max_depth=None)\n",
    "model.fit(X_train, y_train)\n",
    "# Print the decision tree\n",
    "def print_decision_tree_array(tree_dict, values=None, indent=0):\n",
    "    \"\"\"\n",
    "    Recursively print the decision tree dictionary with proper indentation.\n",
    "    \"\"\"\n",
    "    # Base case: leaf node\n",
    "    if 'class' in tree_dict:\n",
    "        print(f\"{'  ' * indent}Output: {tree_dict['class']}\")\n",
    "        return\n",
    "    \n",
    "    # Non-leaf node\n",
    "    values_str = f\"{tree_dict['feature_index']}, values: {np.array2string(tree_dict['values'])}\"\n",
    "    print(f\"{'  ' * indent}Feature index: {values_str}\")\n",
    "    \n",
    "    # Print subtrees\n",
    "    for i, subtree in enumerate(tree_dict['subtree']):\n",
    "        value = tree_dict['values'][i] if 'values' in tree_dict else None\n",
    "        value_str = f\"value {value}\" if value is not None else \"\"\n",
    "        print(f\"{'  ' * (indent + 1)}{value_str}:\", end=\" \")\n",
    "        print_decision_tree_array(subtree, values + [value] if values else [value], indent + 1)\n",
    "print(\"Decision Tree: \")\n",
    "print_decision_tree_array(model.tree)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
