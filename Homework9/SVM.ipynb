{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 9\n",
    "12110418 \n",
    "庄子鲲\n",
    "## Problem 1\n",
    "Using Python and Numpy, write a class named SVMClassifier, which implements the SVM algorithm having slack variables and kernels such as Polynomial,Gaussian, and Sigmoid (using cvxopt package to solve the quadratic programing problem for Lagrange multipliers)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a svm with slack variables and kernels:\n",
    "Optimize function:\n",
    "$$\n",
    "\\max_{a_i\\ge 0}\\{\\sum_{i=1}^{N}\\alpha _i-\\frac{1}{2}\\sum_{i,j=1}^{N}t^{(i)}t^{(j)}\\alpha^i \\alpha ^j K(\\mathbf x^{(i)},\\mathbf x^{(j)}) \\}\\\\\n",
    "$$\n",
    "where\n",
    "$$\n",
    "0\\le \\alpha^{(i)}\\le -\\xi^{(i)}\\\\\n",
    "\\xi^{(i)}\\ge0\n",
    "$$\n",
    "Linear kernel:\n",
    "$$\n",
    "K(\\mathbf x^{(i)},\\mathbf x^{(j)})=\\mathbf x^{(i)^{T}} \\mathbf x^{(j)}\n",
    "$$\n",
    "$d^{\\text {th}}$ degree of Polynomial kernel:\n",
    "$$\n",
    "K(\\mathbf x^{(i)},\\mathbf x^{(j)})=(\\mathbf x^{(i)}\\mathbf x^{(j)}+1)^d\n",
    "$$\n",
    "Gaussian kernel:\n",
    "$$\n",
    "K(\\mathbf x^{(i)},\\mathbf x^{(j)})=\\exp(-\\frac{||\\mathbf x^{(i)}-\\mathbf x^{(j)}||^2}{2\\sigma ^2})\n",
    "$$\n",
    "Sigmoid kernel:\n",
    "$$\n",
    "K(\\mathbf x^{(i)},\\mathbf x^{(j)})=\\tanh (\\beta (\\mathbf x^{(i)^{T}}\\mathbf x^{(j)}))\n",
    "$$\n",
    "Output:\n",
    "$$\n",
    "y=\\text {sign}[b+\\mathbf x\\cdot (\\sum_{i=1}^{(N)}\\alpha_it^{(i)}\\mathbf x^{(i)})]\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from cvxopt import matrix, solvers\n",
    "\n",
    "class SVMClassifier:\n",
    "    def __init__(self, kernel='linear', degree=3, alpha_threshold=1e-5,gamma=None, coef0=0.0, C=1.0):\n",
    "        self.kernel = kernel\n",
    "        self.degree = degree\n",
    "        self.gamma = gamma\n",
    "        self.coef0 = coef0\n",
    "        self.C = C\n",
    "        self.alpha_threshold = alpha_threshold\n",
    "        self.alpha = None\n",
    "        self.support_vectors = None\n",
    "        self.support_vector_labels = None\n",
    "        self.bias = None\n",
    "\n",
    "    def linear_kernel(self, x1, x2):\n",
    "        return np.dot(x1, x2.T)\n",
    "    \n",
    "    def poly_kernel(self, x1, x2):\n",
    "        return (np.dot(x1, x2.T) + self.coef0)**self.degree\n",
    "    \n",
    "    def rbf_kernel(self, x1, x2):\n",
    "        return np.exp(-((x1[:, np.newaxis] - x2)**2).sum(axis=2)/(2 * self.gamma**2))\n",
    "    \n",
    "    def sigmoid_kernel(self, x1, x2):\n",
    "        return np.tanh(self.gamma * np.dot(x1, x2.T) + self.coef0)\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        if self.kernel == 'linear':\n",
    "            self.K = self.linear_kernel(X, X)\n",
    "        elif self.kernel == 'poly':\n",
    "            self.K = self.poly_kernel(X, X)\n",
    "        elif self.kernel == 'rbf':\n",
    "            if self.gamma is None:\n",
    "                self.gamma = 1.0 / X.shape[1]  # Default gamma\n",
    "            self.K = self.rbf_kernel(X, X)\n",
    "        elif self.kernel == 'sigmoid':\n",
    "            self.K = self.sigmoid_kernel(X, X)\n",
    "\n",
    "        n_samples, n_features = X.shape\n",
    "        P = matrix(np.outer(y, y) * self.K, tc='d')\n",
    "        q = matrix(-np.ones(n_samples), tc='d')\n",
    "        G = matrix(np.vstack((np.eye(n_samples), -np.eye(n_samples))), tc='d')\n",
    "        h = matrix(np.hstack((self.C * np.ones(n_samples), np.zeros(n_samples))), tc='d')\n",
    "        A = matrix(y.reshape(1, -1), tc='d')\n",
    "        b = matrix(0.0, tc='d')\n",
    "\n",
    "        # Solve the quadratic programming problem\n",
    "        solution = solvers.qp(P, q, G, h, A, b)\n",
    "\n",
    "        # Extract Lagrange multipliers from the solution\n",
    "        self.alpha = np.array(solution['x']).flatten()\n",
    "        # Support vectors have non-zero Lagrange multipliers\n",
    "        sv_indices = np.where(self.alpha > self.alpha_threshold)[0]\n",
    "        self.support_vectors = X[sv_indices]\n",
    "        self.support_vector_labels = y[sv_indices]\n",
    "        print('number of support vector: ',self.support_vectors.shape[0])\n",
    "        self.alpha = self.alpha.reshape(-1,1)\n",
    "        self.alpha=self.alpha[sv_indices]\n",
    "        # Compute the bias term\n",
    "        if self.kernel == 'linear':\n",
    "            self.bias = np.mean(self.support_vector_labels-np.dot(self.linear_kernel(self.support_vectors,self.support_vectors), (self.alpha * self.support_vector_labels)))     \n",
    "        elif self.kernel == 'poly':\n",
    "            self.bias = np.mean(self.support_vector_labels-np.dot(self.poly_kernel(self.support_vectors,self.support_vectors), (self.alpha * self.support_vector_labels)))\n",
    "        elif self.kernel=='rbf':\n",
    "            self.bias = np.mean(self.support_vector_labels-np.dot(self.rbf_kernel(self.support_vectors,self.support_vectors),(self.alpha * self.support_vector_labels)))\n",
    "        elif self.kernel=='sigmoid':\n",
    "            self.bias = np.mean(self.support_vector_labels-np.dot(self.sigmoid_kernel(self.support_vectors,self.support_vectors),(self.alpha * self.support_vector_labels)))\n",
    "        \n",
    "        \n",
    "    def predict(self, X):\n",
    "        if self.kernel == 'linear':\n",
    "            decision_function = np.dot(self.linear_kernel(X,self.support_vectors), (self.alpha * self.support_vector_labels)) + self.bias\n",
    "        elif self.kernel == 'poly':\n",
    "            decision_function = np.dot(self.poly_kernel(X,self.support_vectors), (self.alpha * self.support_vector_labels)) + self.bias\n",
    "        elif self.kernel == 'rbf':\n",
    "            decision_function = np.dot(self.rbf_kernel(X,self.support_vectors),(self.alpha * self.support_vector_labels)) + self.bias\n",
    "        elif self.kernel == 'sigmoid':\n",
    "            decision_function = np.dot(self.sigmoid_kernel(X,self.support_vectors),(self.alpha * self.support_vector_labels)) + self.bias\n",
    "\n",
    "        # Predict using the sign of the decision function\n",
    "        return np.sign(decision_function)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2\n",
    "Consider the dataset of letter recognition (named letter-recognition.data).The dataset has 20,000 samples, for which the first column indicates theclass(A~Z,totally 26 classes), and the rest columns indicate 16 features as described in the following table. For this dataset, use SVM to do a binary classification for letter‘C’or non-‘C’class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Linear Kernel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4800 training data is used to train\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -5.7559e+02 -1.0103e+04  6e+04  3e+00  2e-11\n",
      " 1: -3.7733e+02 -5.4811e+03  9e+03  3e-01  1e-11\n",
      " 2: -3.3897e+02 -2.5982e+03  4e+03  1e-01  1e-11\n",
      " 3: -3.0722e+02 -2.1627e+03  3e+03  8e-02  1e-11\n",
      " 4: -2.7483e+02 -1.3717e+03  2e+03  4e-02  9e-12\n",
      " 5: -2.6111e+02 -9.1949e+02  1e+03  2e-02  8e-12\n",
      " 6: -2.5598e+02 -6.7103e+02  6e+02  1e-02  8e-12\n",
      " 7: -2.6106e+02 -4.9233e+02  3e+02  5e-03  8e-12\n",
      " 8: -2.6418e+02 -4.4184e+02  2e+02  3e-03  8e-12\n",
      " 9: -2.6664e+02 -4.0698e+02  2e+02  2e-03  8e-12\n",
      "10: -2.6859e+02 -3.8642e+02  1e+02  1e-03  8e-12\n",
      "11: -2.7228e+02 -3.6607e+02  1e+02  1e-03  9e-12\n",
      "12: -2.7326e+02 -3.5136e+02  8e+01  5e-04  9e-12\n",
      "13: -2.7617e+02 -3.3622e+02  6e+01  2e-04  1e-11\n",
      "14: -2.8080e+02 -3.2461e+02  5e+01  1e-04  1e-11\n",
      "15: -2.8231e+02 -3.2101e+02  4e+01  8e-05  1e-11\n",
      "16: -2.8468e+02 -3.1531e+02  3e+01  4e-05  1e-11\n",
      "17: -2.8627e+02 -3.1193e+02  3e+01  3e-05  1e-11\n",
      "18: -2.8781e+02 -3.0832e+02  2e+01  1e-05  1e-11\n",
      "19: -2.8939e+02 -3.0568e+02  2e+01  5e-06  1e-11\n",
      "20: -2.8961e+02 -3.0534e+02  2e+01  4e-06  1e-11\n",
      "21: -2.9177e+02 -3.0240e+02  1e+01  2e-06  1e-11\n",
      "22: -2.9266e+02 -3.0131e+02  9e+00  1e-06  1e-11\n",
      "23: -2.9270e+02 -3.0118e+02  8e+00  1e-06  1e-11\n",
      "24: -2.9405e+02 -2.9938e+02  5e+00  5e-07  1e-11\n",
      "25: -2.9475e+02 -2.9855e+02  4e+00  3e-07  1e-11\n",
      "26: -2.9509e+02 -2.9812e+02  3e+00  2e-07  1e-11\n",
      "27: -2.9573e+02 -2.9739e+02  2e+00  5e-08  1e-11\n",
      "28: -2.9604e+02 -2.9702e+02  1e+00  1e-08  1e-11\n",
      "29: -2.9625e+02 -2.9680e+02  5e-01  9e-10  1e-11\n",
      "30: -2.9641e+02 -2.9663e+02  2e-01  1e-14  1e-11\n",
      "31: -2.9650e+02 -2.9654e+02  4e-02  4e-14  1e-11\n",
      "32: -2.9651e+02 -2.9653e+02  1e-02  1e-14  1e-11\n",
      "33: -2.9652e+02 -2.9652e+02  2e-04  2e-14  1e-11\n",
      "Optimal solution found.\n",
      "number of support vector:  305\n",
      "1200 testing data to be test\n",
      "accuracy: 0.9775\n"
     ]
    }
   ],
   "source": [
    "svm_linear = SVMClassifier(kernel='linear', gamma=0.1, C=1.0)\n",
    "# Load data from the file\n",
    "data = np.loadtxt(\"letter-recognition.data\", dtype=str, delimiter=',')\n",
    "# Extract labels and features\n",
    "y = np.array([1 if label == \"C\" else -1 for label in data[:6000, 0]])\n",
    "y=y.reshape(-1,1)\n",
    "X = data[:6000, 1:].astype(int)\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Split the data into training and testing sets (80% training, 20% testing)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "print(X_train.shape[0],'training data is used to train')\n",
    "svm_linear.fit(X_train, y_train)\n",
    "predictions = svm_linear.predict(X_test)\n",
    "print(X_test.shape[0],'testing data to be test')\n",
    "print('accuracy:', accuracy_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Polynomial Kernel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -1.5936e+02 -9.3055e+03  5e+04  3e+00  5e-06\n",
      " 1: -1.0231e+02 -5.0491e+03  1e+04  4e-01  5e-06\n",
      " 2: -6.0150e+01 -2.1735e+03  4e+03  1e-01  3e-06\n",
      " 3: -3.5221e+01 -1.3557e+03  2e+03  8e-02  2e-06\n",
      " 4: -2.2088e+01 -9.2339e+02  2e+03  5e-02  1e-06\n",
      " 5: -1.1831e+01 -5.3100e+02  9e+02  2e-02  8e-07\n",
      " 6: -4.5851e+00 -4.1067e+02  7e+02  1e-02  5e-07\n",
      " 7: -2.1619e-01 -2.4166e+01  4e+01  8e-04  1e-07\n",
      " 8: -2.1936e-02 -3.2150e+00  5e+00  9e-05  2e-08\n",
      " 9: -6.3982e-03 -6.7029e-01  1e+00  2e-05  4e-09\n",
      "10: -3.3485e-03 -2.2923e-01  3e-01  5e-06  1e-09\n",
      "11: -1.9474e-03 -1.0172e-01  1e-01  2e-06  6e-10\n",
      "12: -1.1739e-03 -4.8356e-02  7e-02  6e-07  3e-10\n",
      "13: -7.1427e-04 -2.1186e-02  3e-02  2e-07  2e-10\n",
      "14: -4.3522e-04 -1.0390e-02  1e-02  1e-07  9e-11\n",
      "15: -3.0935e-04 -5.0005e-03  6e-03  4e-08  7e-11\n",
      "16: -2.5519e-04 -2.7808e-03  3e-03  2e-08  6e-11\n",
      "17: -2.0348e-04 -1.6727e-03  2e-03  5e-09  5e-11\n",
      "18: -2.9682e-04 -1.0811e-03  9e-04  2e-09  5e-11\n",
      "19: -2.7905e-04 -9.2225e-04  7e-04  8e-10  6e-11\n",
      "20: -3.8611e-04 -6.3344e-04  3e-04  2e-10  6e-11\n",
      "21: -4.0264e-04 -5.6306e-04  2e-04  3e-12  7e-11\n",
      "22: -4.4928e-04 -4.9495e-04  5e-05  7e-13  7e-11\n",
      "23: -4.5854e-04 -4.8015e-04  2e-05  2e-16  8e-11\n",
      "24: -4.6786e-04 -4.6958e-04  2e-06  2e-16  8e-11\n",
      "25: -4.6866e-04 -4.6869e-04  3e-08  2e-16  8e-11\n",
      "Optimal solution found.\n",
      "number of support vector:  125\n",
      "accuracy: 0.9891666666666666\n"
     ]
    }
   ],
   "source": [
    "svm_poly = SVMClassifier(kernel='poly', alpha_threshold=1e-10,degree=3, gamma=0.1, C=1.0)\n",
    "svm_poly.fit(X_train, y_train)\n",
    "predictions = svm_poly.predict(X_test)\n",
    "print('accuracy:', accuracy_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Guassian Kernel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -3.3246e+02 -6.8029e+03  3e+04  2e+00  2e-16\n",
      " 1: -2.9590e+02 -2.8883e+03  3e+03  7e-02  4e-16\n",
      " 2: -2.8817e+02 -4.3812e+02  2e+02  5e-04  5e-16\n",
      " 3: -2.8857e+02 -2.9053e+02  2e+00  6e-06  1e-16\n",
      " 4: -2.8871e+02 -2.8877e+02  5e-02  8e-08  6e-17\n",
      " 5: -2.8872e+02 -2.8872e+02  2e-03  8e-10  2e-17\n",
      " 6: -2.8872e+02 -2.8872e+02  2e-05  8e-12  7e-17\n",
      "Optimal solution found.\n",
      "number of support vector:  4800\n",
      "accuracy: 0.9666666666666667\n"
     ]
    }
   ],
   "source": [
    "svm_rbf = SVMClassifier(kernel='rbf',gamma=0.1, C=1.0)\n",
    "svm_rbf.fit(X_train, y_train)\n",
    "predictions = svm_rbf.predict(X_test)\n",
    "print('accuracy:', accuracy_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Sigmoid Kernel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -7.5199e+02 -9.9600e+03  5e+04  3e+00  3e-13\n",
      " 1: -4.2338e+02 -5.1953e+03  6e+03  1e-01  4e-13\n",
      " 2: -3.9141e+02 -6.9147e+02  3e+02  3e-04  3e-14\n",
      " 3: -3.9189e+02 -3.9502e+02  3e+00  3e-06  1e-14\n",
      " 4: -3.9200e+02 -3.9203e+02  3e-02  3e-08  2e-14\n",
      " 5: -3.9200e+02 -3.9200e+02  3e-04  3e-10  2e-14\n",
      "Optimal solution found.\n",
      "number of support vector:  4800\n",
      "accuracy: 0.9658333333333333\n"
     ]
    }
   ],
   "source": [
    "svm_sigmoid = SVMClassifier(kernel='sigmoid',gamma=0.1, C=1.0)\n",
    "svm_sigmoid.fit(X_train, y_train)\n",
    "predictions = svm_sigmoid.predict(X_test)\n",
    "print('accuracy:', accuracy_score(y_test, predictions))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
