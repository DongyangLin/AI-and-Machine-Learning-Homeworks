{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 9\n",
    "12110418 \n",
    "庄子鲲\n",
    "## Problem 1\n",
    "Using Python and Numpy, write a class named SVMClassifier, which implements the SVM algorithm having slack variables and kernels such as Polynomial,Gaussian, and Sigmoid (using cvxopt package to solve the quadratic programing problem for Lagrange multipliers)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a svm with slack variables and kernels:\n",
    "Optimize function:\n",
    "$$\n",
    "\\max_{a_i\\ge 0}\\{\\sum_{i=1}^{N}\\alpha _i-\\frac{1}{2}\\sum_{i,j=1}^{N}t^{(i)}t^{(j)}\\alpha^i \\alpha ^j K(\\mathbf x^{(i)},\\mathbf x^{(j)}) \\}\\\\\n",
    "$$\n",
    "where\n",
    "$$\n",
    "0\\le \\alpha^{(i)}\\le -\\xi^{(i)}\\\\\n",
    "\\xi^{(i)}\\ge0\n",
    "$$\n",
    "Linear kernel:\n",
    "$$\n",
    "K(\\mathbf x^{(i)},\\mathbf x^{(j)})=\\mathbf x^{(i)^{T}} \\mathbf x^{(j)}\n",
    "$$\n",
    "$d^{\\text {th}}$ degree of Polynomial kernel:\n",
    "$$\n",
    "K(\\mathbf x^{(i)},\\mathbf x^{(j)})=(\\mathbf x^{(i)}\\mathbf x^{(j)}+1)^d\n",
    "$$\n",
    "Gaussian kernel:\n",
    "$$\n",
    "K(\\mathbf x^{(i)},\\mathbf x^{(j)})=\\exp(-\\frac{||\\mathbf x^{(i)}-\\mathbf x^{(j)}||^2}{2\\sigma ^2})\n",
    "$$\n",
    "Sigmoid kernel:\n",
    "$$\n",
    "K(\\mathbf x^{(i)},\\mathbf x^{(j)})=\\tanh (\\beta (\\mathbf x^{(i)^{T}}\\mathbf x^{(j)}))\n",
    "$$\n",
    "Output:\n",
    "$$\n",
    "y=\\text {sign}[b+\\mathbf x\\cdot (\\sum_{i=1}^{(N)}\\alpha_it^{(i)}\\mathbf x^{(i)})]\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from cvxopt import matrix, solvers\n",
    "\n",
    "class SVMClassifier:\n",
    "    def __init__(self, kernel='linear', degree=3, alpha_threshold=1e-5,gamma=None, coef0=0.0, C=1.0):\n",
    "        self.kernel = kernel\n",
    "        self.degree = degree\n",
    "        self.gamma = gamma\n",
    "        self.coef0 = coef0\n",
    "        self.C = C\n",
    "        self.alpha_threshold = alpha_threshold\n",
    "        self.alpha = None\n",
    "        self.support_vectors = None\n",
    "        self.support_vector_labels = None\n",
    "        self.bias = None\n",
    "\n",
    "    def linear_kernel(self, x1, x2):\n",
    "        return np.dot(x1, x2.T)\n",
    "    \n",
    "    def poly_kernel(self, x1, x2):\n",
    "        return (np.dot(x1, x2.T) + self.coef0)**self.degree\n",
    "    \n",
    "    def rbf_kernel(self, x1, x2):\n",
    "        return np.exp(-((x1[:, np.newaxis] - x2)**2).sum(axis=2)/(2 * self.gamma**2))\n",
    "    \n",
    "    def sigmoid_kernel(self, x1, x2):\n",
    "        return np.tanh(self.gamma * np.dot(x1, x2.T) + self.coef0)\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        if self.kernel == 'linear':\n",
    "            self.K = self.linear_kernel(X, X)\n",
    "        elif self.kernel == 'poly':\n",
    "            self.K = self.poly_kernel(X, X)\n",
    "        elif self.kernel == 'rbf':\n",
    "            if self.gamma is None:\n",
    "                self.gamma = 1.0 / X.shape[1]  # Default gamma\n",
    "            self.K = self.rbf_kernel(X, X)\n",
    "        elif self.kernel == 'sigmoid':\n",
    "            self.K = self.sigmoid_kernel(X, X)\n",
    "\n",
    "        n_samples, n_features = X.shape\n",
    "        P = matrix(np.outer(y, y) * self.K, tc='d')\n",
    "        q = matrix(-np.ones(n_samples), tc='d')\n",
    "        G = matrix(np.vstack((np.eye(n_samples), -np.eye(n_samples))), tc='d')\n",
    "        h = matrix(np.hstack((self.C * np.ones(n_samples), np.zeros(n_samples))), tc='d')\n",
    "        A = matrix(y.reshape(1, -1), tc='d')\n",
    "        b = matrix(0.0, tc='d')\n",
    "\n",
    "        # Solve the quadratic programming problem\n",
    "        solution = solvers.qp(P, q, G, h, A, b)\n",
    "\n",
    "        # Extract Lagrange multipliers from the solution\n",
    "        self.alpha = np.array(solution['x']).flatten()\n",
    "        # Support vectors have non-zero Lagrange multipliers\n",
    "        sv_indices = np.where(self.alpha > self.alpha_threshold)[0]\n",
    "        self.support_vectors = X[sv_indices]\n",
    "        self.support_vector_labels = y[sv_indices]\n",
    "        print('number of support vector: ',self.support_vectors.shape[0])\n",
    "        self.alpha = self.alpha.reshape(-1,1)\n",
    "        self.alpha=self.alpha[sv_indices]\n",
    "        # Compute the bias term\n",
    "        if self.kernel == 'linear':\n",
    "            self.bias = np.mean(self.support_vector_labels-np.dot(self.linear_kernel(self.support_vectors,self.support_vectors), (self.alpha * self.support_vector_labels)))     \n",
    "        elif self.kernel == 'poly':\n",
    "            self.bias = np.mean(self.support_vector_labels-np.dot(self.poly_kernel(self.support_vectors,self.support_vectors), (self.alpha * self.support_vector_labels)))\n",
    "        elif self.kernel=='rbf':\n",
    "            self.bias = np.mean(self.support_vector_labels-np.dot(self.rbf_kernel(self.support_vectors,self.support_vectors),(self.alpha * self.support_vector_labels)))\n",
    "        elif self.kernel=='sigmoid':\n",
    "            self.bias = np.mean(self.support_vector_labels-np.dot(self.sigmoid_kernel(self.support_vectors,self.support_vectors),(self.alpha * self.support_vector_labels)))\n",
    "        \n",
    "        \n",
    "    def predict(self, X):\n",
    "        if self.kernel == 'linear':\n",
    "            decision_function = np.dot(self.linear_kernel(X,self.support_vectors), (self.alpha * self.support_vector_labels)) + self.bias\n",
    "        elif self.kernel == 'poly':\n",
    "            decision_function = np.dot(self.poly_kernel(X,self.support_vectors), (self.alpha * self.support_vector_labels)) + self.bias\n",
    "        elif self.kernel == 'rbf':\n",
    "            decision_function = np.dot(self.rbf_kernel(X,self.support_vectors),(self.alpha * self.support_vector_labels)) + self.bias\n",
    "        elif self.kernel == 'sigmoid':\n",
    "            decision_function = np.dot(self.sigmoid_kernel(X,self.support_vectors),(self.alpha * self.support_vector_labels)) + self.bias\n",
    "\n",
    "        # Predict using the sign of the decision function\n",
    "        return np.sign(decision_function)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2\n",
    "Consider the dataset of letter recognition (named letter-recognition.data).The dataset has 20,000 samples, for which the first column indicates theclass(A~Z,totally 26 classes), and the rest columns indicate 16 features as described in the following table. For this dataset, use SVM to do a binary classification for letter‘C’or non-‘C’class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Linear Kernel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2400 training data is used to train\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -2.7929e+02 -5.0035e+03  3e+04  3e+00  1e-11\n",
      " 1: -1.8392e+02 -2.7396e+03  5e+03  3e-01  8e-12\n",
      " 2: -1.6411e+02 -1.2470e+03  2e+03  1e-01  6e-12\n",
      " 3: -1.4639e+02 -1.0070e+03  1e+03  8e-02  5e-12\n",
      " 4: -1.3218e+02 -6.3803e+02  8e+02  4e-02  5e-12\n",
      " 5: -1.2359e+02 -4.6493e+02  5e+02  2e-02  4e-12\n",
      " 6: -1.2221e+02 -2.8184e+02  2e+02  8e-03  4e-12\n",
      " 7: -1.2275e+02 -2.3608e+02  1e+02  5e-03  4e-12\n",
      " 8: -1.2552e+02 -1.8358e+02  7e+01  2e-03  4e-12\n",
      " 9: -1.2631e+02 -1.8003e+02  6e+01  1e-03  4e-12\n",
      "10: -1.2643e+02 -1.7016e+02  5e+01  6e-04  4e-12\n",
      "11: -1.2787e+02 -1.6118e+02  4e+01  3e-04  4e-12\n",
      "12: -1.2855e+02 -1.6002e+02  3e+01  3e-04  4e-12\n",
      "13: -1.3116e+02 -1.5410e+02  2e+01  2e-04  4e-12\n",
      "14: -1.3132e+02 -1.5282e+02  2e+01  1e-04  4e-12\n",
      "15: -1.3368e+02 -1.4786e+02  1e+01  7e-05  4e-12\n",
      "16: -1.3419e+02 -1.4700e+02  1e+01  5e-05  4e-12\n",
      "17: -1.3519e+02 -1.4465e+02  1e+01  2e-05  4e-12\n",
      "18: -1.3601e+02 -1.4316e+02  7e+00  1e-05  4e-12\n",
      "19: -1.3648e+02 -1.4229e+02  6e+00  1e-05  4e-12\n",
      "20: -1.3710e+02 -1.4122e+02  4e+00  5e-06  4e-12\n",
      "21: -1.3737e+02 -1.4060e+02  3e+00  2e-06  4e-12\n",
      "22: -1.3769e+02 -1.4017e+02  2e+00  1e-06  4e-12\n",
      "23: -1.3831e+02 -1.3940e+02  1e+00  3e-07  4e-12\n",
      "24: -1.3837e+02 -1.3933e+02  1e+00  2e-07  4e-12\n",
      "25: -1.3839e+02 -1.3928e+02  9e-01  1e-07  4e-12\n",
      "26: -1.3864e+02 -1.3901e+02  4e-01  2e-08  4e-12\n",
      "27: -1.3879e+02 -1.3885e+02  7e-02  2e-09  4e-12\n",
      "28: -1.3882e+02 -1.3882e+02  3e-03  5e-11  4e-12\n",
      "29: -1.3882e+02 -1.3882e+02  3e-05  5e-13  4e-12\n",
      "Optimal solution found.\n",
      "number of support vector:  148\n",
      "600 testing data to be test\n",
      "accuracy: 0.975\n"
     ]
    }
   ],
   "source": [
    "svm_linear = SVMClassifier(kernel='linear', gamma=0.1, C=1.0)\n",
    "# Load data from the file\n",
    "data = np.loadtxt(\"letter-recognition.data\", dtype=str, delimiter=',')\n",
    "# Extract labels and features\n",
    "y = np.array([1 if label == \"C\" else -1 for label in data[:3000, 0]])\n",
    "y=y.reshape(-1,1)\n",
    "X = data[:3000, 1:].astype(int)\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Split the data into training and testing sets (80% training, 20% testing)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "print(X_train.shape[0],'training data is used to train')\n",
    "svm_linear.fit(X_train, y_train)\n",
    "predictions = svm_linear.predict(X_test)\n",
    "print(X_test.shape[0],'testing data to be test')\n",
    "print('accuracy:', accuracy_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Polynomial Kernel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -5.7607e+01 -3.9301e+03  2e+04  2e+00  2e-06\n",
      " 1: -3.4795e+01 -1.8461e+03  3e+03  3e-01  2e-06\n",
      " 2: -1.7125e+01 -8.9159e+02  2e+03  1e-01  1e-06\n",
      " 3: -1.0456e+01 -5.6253e+02  1e+03  7e-02  1e-06\n",
      " 4: -5.4415e-01 -2.6114e+02  4e+02  2e-02  4e-07\n",
      " 5: -7.7377e-03 -1.2862e+01  2e+01  9e-04  4e-08\n",
      " 6: -3.0346e-03 -1.5850e+00  2e+00  1e-04  5e-09\n",
      " 7: -1.9960e-03 -2.9432e-01  4e-01  2e-05  1e-09\n",
      " 8: -1.0008e-03 -7.8224e-02  1e-01  4e-06  3e-10\n",
      " 9: -4.8212e-04 -3.4432e-02  5e-02  1e-06  2e-10\n",
      "10: -2.5371e-04 -1.1747e-02  2e-02  4e-07  7e-11\n",
      "11: -1.4527e-04 -4.7776e-03  6e-03  1e-07  4e-11\n",
      "12: -7.8857e-05 -2.2889e-03  3e-03  6e-08  2e-11\n",
      "13: -4.8948e-05 -9.7840e-04  1e-03  2e-08  1e-11\n",
      "14: -5.3992e-05 -4.1926e-04  5e-04  7e-09  9e-12\n",
      "15: -5.8278e-05 -1.7974e-04  1e-04  6e-10  1e-11\n",
      "16: -7.2909e-05 -1.3397e-04  6e-05  2e-10  9e-12\n",
      "17: -8.1590e-05 -1.0639e-04  2e-05  2e-16  1e-11\n",
      "18: -8.9078e-05 -9.4524e-05  5e-06  2e-16  1e-11\n",
      "19: -9.0815e-05 -9.2149e-05  1e-06  2e-16  1e-11\n",
      "20: -9.1368e-05 -9.1425e-05  6e-08  2e-16  1e-11\n",
      "Optimal solution found.\n",
      "number of support vector:  114\n",
      "accuracy: 0.9833333333333333\n"
     ]
    }
   ],
   "source": [
    "svm_poly = SVMClassifier(kernel='poly', alpha_threshold=1e-10,degree=3, gamma=0.1, C=1.0)\n",
    "svm_poly.fit(X_train, y_train)\n",
    "predictions = svm_poly.predict(X_test)\n",
    "print('accuracy:', accuracy_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Guassian Kernel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -1.6213e+02 -3.3969e+03  1e+04  2e+00  1e-16\n",
      " 1: -1.4415e+02 -1.4385e+03  2e+03  7e-02  7e-16\n",
      " 2: -1.4029e+02 -2.1626e+02  8e+01  5e-04  6e-16\n",
      " 3: -1.4046e+02 -1.4139e+02  9e-01  6e-06  1e-16\n",
      " 4: -1.4052e+02 -1.4053e+02  2e-02  8e-08  1e-16\n",
      " 5: -1.4052e+02 -1.4052e+02  3e-04  8e-10  8e-17\n",
      " 6: -1.4052e+02 -1.4052e+02  3e-06  8e-12  5e-17\n",
      "Optimal solution found.\n",
      "number of support vector:  2400\n",
      "accuracy: 0.9566666666666667\n"
     ]
    }
   ],
   "source": [
    "svm_rbf = SVMClassifier(kernel='rbf',gamma=0.1, C=1.0)\n",
    "svm_rbf.fit(X_train, y_train)\n",
    "predictions = svm_rbf.predict(X_test)\n",
    "print('accuracy:', accuracy_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Sigmoid Kernel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -3.6496e+02 -4.9750e+03  3e+04  3e+00  2e-13\n",
      " 1: -2.0534e+02 -2.5896e+03  3e+03  1e-01  2e-13\n",
      " 2: -1.8972e+02 -3.3922e+02  2e+02  3e-04  2e-14\n",
      " 3: -1.8995e+02 -1.9150e+02  2e+00  3e-06  4e-14\n",
      " 4: -1.9000e+02 -1.9002e+02  2e-02  3e-08  2e-14\n",
      " 5: -1.9000e+02 -1.9000e+02  2e-04  3e-10  2e-14\n",
      "Optimal solution found.\n",
      "number of support vector:  2400\n",
      "accuracy: 0.9566666666666667\n"
     ]
    }
   ],
   "source": [
    "svm_sigmoid = SVMClassifier(kernel='sigmoid',gamma=0.1, C=1.0)\n",
    "svm_sigmoid.fit(X_train, y_train)\n",
    "predictions = svm_sigmoid.predict(X_test)\n",
    "print('accuracy:', accuracy_score(y_test, predictions))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
