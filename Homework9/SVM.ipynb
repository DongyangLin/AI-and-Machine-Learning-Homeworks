{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 9\n",
    "12110418 \n",
    "庄子鲲\n",
    "## Problem 1\n",
    "Using Python and Numpy, write a class named SVMClassifier, which implements the SVM algorithm having slack variables and kernels such as Polynomial,Gaussian, and Sigmoid (using cvxopt package to solve the quadratic programing problem for Lagrange multipliers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from cvxopt import matrix, solvers\n",
    "\n",
    "class SVMClassifier:\n",
    "    def __init__(self, kernel='linear', degree=3, gamma=None, coef0=0.0, C=1.0):\n",
    "        self.kernel = kernel\n",
    "        self.degree = degree\n",
    "        self.gamma = gamma\n",
    "        self.coef0 = coef0\n",
    "        self.C = C\n",
    "        self.alpha = None\n",
    "        self.support_vectors = None\n",
    "        self.support_vector_labels = None\n",
    "        self.bias = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        if self.kernel == 'linear':\n",
    "            self.K = np.dot(X, X.T)\n",
    "        elif self.kernel == 'poly':\n",
    "            self.K = (np.dot(X, X.T) + self.coef0)**self.degree\n",
    "        elif self.kernel == 'rbf':\n",
    "            if self.gamma is None:\n",
    "                self.gamma = 1.0 / X.shape[1]  # Default gamma\n",
    "            self.K = np.exp(-self.gamma * ((X[:, np.newaxis] - X)**2).sum(axis=2))\n",
    "        elif self.kernel == 'sigmoid':\n",
    "            self.K = np.tanh(self.gamma * np.dot(X, X.T) + self.coef0)\n",
    "\n",
    "        n_samples, n_features = X.shape\n",
    "        P = matrix(np.outer(y, y) * self.K, tc='d')\n",
    "        q = matrix(-np.ones(n_samples), tc='d')\n",
    "        G = matrix(np.vstack((np.eye(n_samples), -np.eye(n_samples))), tc='d')\n",
    "        h = matrix(np.hstack((self.C * np.ones(n_samples), np.zeros(n_samples))), tc='d')\n",
    "        A = matrix(y.reshape(1, -1), tc='d')\n",
    "        b = matrix(0.0, tc='d')\n",
    "\n",
    "        # Solve the quadratic programming problem\n",
    "        solution = solvers.qp(P, q, G, h, A, b)\n",
    "\n",
    "        # Extract Lagrange multipliers from the solution\n",
    "        self.alpha = np.array(solution['x']).flatten()\n",
    "\n",
    "        # Support vectors have non-zero Lagrange multipliers\n",
    "        sv_indices = np.where(self.alpha > 1e-5)[0]\n",
    "        self.support_vectors = X[sv_indices]\n",
    "        self.support_vector_labels = y[sv_indices]\n",
    "\n",
    "        # Compute the bias term\n",
    "        if self.kernel == 'linear':\n",
    "            self.bias = np.mean(self.support_vector_labels - np.dot(self.alpha * self.support_vector_labels, self.K[sv_indices]))\n",
    "        else:\n",
    "            self.bias = 0  # Bias term not implemented for non-linear kernels\n",
    "\n",
    "    def predict(self, X):\n",
    "        if self.kernel == 'linear':\n",
    "            decision_function = np.dot(self.alpha * self.support_vector_labels, np.dot(X, self.support_vectors.T)) + self.bias\n",
    "        elif self.kernel == 'poly':\n",
    "            decision_function = np.dot(self.alpha * self.support_vector_labels, (np.dot(X, self.support_vectors.T) + self.coef0)**self.degree) + self.bias\n",
    "        elif self.kernel == 'rbf':\n",
    "            decision_function = np.dot(self.alpha * self.support_vector_labels, np.exp(-self.gamma * ((X[:, np.newaxis] - self.support_vectors)**2).sum(axis=2))) + self.bias\n",
    "        elif self.kernel == 'sigmoid':\n",
    "            decision_function = np.dot(self.alpha * self.support_vector_labels, np.tanh(self.gamma * np.dot(X, self.support_vectors.T) + self.coef0)) + self.bias\n",
    "\n",
    "        # Predict using the sign of the decision function\n",
    "        return np.sign(decision_function)\n",
    "\n",
    "# Example usage:\n",
    "# Assuming X_train, y_train, X_test are your training features, training labels, and testing features\n",
    "# svm = SVMClassifier(kernel='rbf', gamma=0.1, C=1.0)\n",
    "# svm.fit(X_train, y_train)\n",
    "# predictions = svm.predict(X_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
